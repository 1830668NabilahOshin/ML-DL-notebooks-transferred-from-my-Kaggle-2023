{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1382412,"sourceType":"datasetVersion","datasetId":806606},{"sourceId":7449449,"sourceType":"datasetVersion","datasetId":4336196}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dropout\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nSTOPWORDS = set(stopwords.words('english'))\n\n\ndef text_to_word_list(text):\n    text = text.split()\n    return text\n\ndef replace_strings(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           u\"\\u00C0-\\u017F\"          #latin\n                           u\"\\u2000-\\u206F\"          #generalPunctuations\n                               \n                           \"]+\", flags=re.UNICODE)\n    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n    #latin_pattern=re.compile('[A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00f6\\u00f8-\\u00ff\\s]*',)\n    \n    text=emoji_pattern.sub(r'', text)\n    text=english_pattern.sub(r'', text)\n\n    return text\n\ndef remove_punctuations(my_str):\n    # define punctuation\n    punctuations = '''````£|¢|Ñ+-*/=EROero৳০১২৩৪৫৬৭৮৯012–34567•89।!()-[]{};:'\"“\\’,<>./?@#$%^&*_~‘—॥”‰⚽️✌�￰৷￰'''\n    \n    no_punct = \"\"\n    for char in my_str:\n        if char not in punctuations:\n            no_punct = no_punct + char\n\n    # display the unpunctuated string\n    return no_punct\n\n\n\ndef joining(text):\n    out=' '.join(text)\n    return out\n\ndef preprocessing(text):\n    out=remove_punctuations(replace_strings(text))\n    return out\n\n\n\ntrain_url = '/kaggle/input/80-20ratiofinal/train8020.csv'\ntest_url = '/kaggle/input/80-20ratiofinal/test8020.csv'\ndf_train = pd.read_csv(train_url)\ndf_test = pd.read_csv(test_url)\nstop_words_df = pd.read_excel('/kaggle/input/bangla-stopwords/stopwords_bangla.xlsx',index_col=False)\nSTOPWORDS = set([word.strip() for word in stop_words_df['words']])\n\ndf_train['Comment'] = df_train.Comment.apply(lambda x: preprocessing(str(x)))\ndf_test['Comment'] = df_test.Comment.apply(lambda x:preprocessing(str(x)))\ndf = pd.concat([df_train,df_test],ignore_index = True)\n\ndf_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\nprint(df_train.head(5))\n\ndef encode(s):\n    d = {\n        \"Code Switching\":0,\n        \"Grammatical\":1,\n        \"Multiple Errors\":2,\n        \"Spelling\":3\n    }\n    if s in d:\n        return d[s]\n    else:\n        return 4\n    \ndf['Category'] = df.Category.apply(lambda x: encode(x))\ndf_train['Category'] = df_train.Category.apply(lambda x: encode(x))\ndf_test['Category'] = df_test.Category.apply(lambda x: encode(x))\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-23T04:19:19.838476Z","iopub.execute_input":"2024-01-23T04:19:19.839106Z","iopub.status.idle":"2024-01-23T04:19:21.387677Z","shell.execute_reply.started":"2024-01-23T04:19:19.839062Z","shell.execute_reply":"2024-01-23T04:19:21.386359Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"      Video ID           Channel name     Time of Publishing  \\\n0  f-rEHLDfXro  ইতিহাসের অনুসন্ধানে 7G  2022-11-26T10:03:44Z   \n1  aLeV04Bz5xk              Channel 24  2023-07-19T13:37:46Z   \n2  zBPoDX9TSXA       Cine Fever Bangla  2022-07-15T17:32:43Z   \n3  sAoIhoP1zhk                ATN News  2023-06-10T16:32:00Z   \n4  jV4gHJYIi34               BigganPiC  2023-02-15T08:00:19Z   \n\n                                               Title          Genre  \\\n0  ১০০ বছর আগে হারিয়ে যাওয়া গ্রাম কেমন ছিল ইতিহাস...  Miscellaneous   \n1  দেশে প্রথমবারের মতো চ্যানেল 24-এর পর্দায় সংবাদ...           News   \n2  বড়লোক অনন্ত বর্ষা কে ধুয়ে দিল পরীমনি!দেখুন কি বলল  Entertainment   \n3  বিএনপি নির্বাচিত হলে কাকে প্রধানমন্ত্রী করা হব...           News   \n4  পৃথিবীর কেন্দ্র Earth core and Earthquake wave...  Miscellaneous   \n\n                                             Comment  Error        Category  \\\n0  হ্যালো যতগুলা পিক দেখালেন সবগুলা তো মনে হচ্ছে ...      1        Spelling   \n1  মানুষের মতো সুন্দর সাবলীল ভাবে এক্সপ্রেশানে কম...      1  Code Switching   \n2  কারোর ব্যক্তিগত জীবনযাপন নিয়ে অনন্ত বা বর্ষা ...      1  Code Switching   \n3  আওয়ামী লীগের জলে কেনো জানতে চাই এই জন্য মামলা ...      1        Spelling   \n4  ভূমিকম্প কি কেন এবং কিভাবে হয় এ নিয়ে বিস্তার...      0         Correct   \n\n                                        Correct Form Unnamed: 9  \n0  হ্যালো যতগুলা পিক দেখালেন সবগুলা তো মনে হচ্ছে ...        NaN  \n1  মানুষের মতো সুন্দর সাবলীল ভাবে অভিব্যক্তিতে কম...        NaN  \n2  কারোর ব্যক্তিগত জীবনযাপন নিয়ে অনন্ত বা বর্ষা ...        NaN  \n3  আওয়ামী লীগের জলে কেনো। জানতে চাই এই জন্য মামলা...        NaN  \n4  ভূমিকম্প কি, কেন এবং কিভাবে হয় এ নিয়ে বিস্তা...        NaN  \n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Attention\nfrom tensorflow.keras.models import Model\n# import tensorflow_addons as tfa\n\n# Sample data and labels (replace with your own dataset)\ndata = df_train['Comment']\nlabels = df_train['Category']\n\n# Tokenize text data\nmax_words = 5000\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(data)\nsequences = tokenizer.texts_to_sequences(data)\nword_index = tokenizer.word_index\nmax_sequence_length = 100  # Adjust this based on your data\n\n# Pad sequences\nX = pad_sequences(sequences, maxlen=max_sequence_length)\n\n# One-hot encode labels\nY = tf.keras.utils.to_categorical(labels, num_classes=5)\n\n# Create LSTM with Attention model\ninput_layer = Input(shape=(max_sequence_length,))\nembedding_layer = Embedding(max_words, 100)(input_layer)\nlstm_layer = LSTM(64, return_sequences=True)(embedding_layer)\n\n# Attention mechanism using TensorFlow Addons\nattention = Attention()([lstm_layer, lstm_layer])\nattention = tf.keras.layers.GlobalAveragePooling1D()(attention)\n\noutput_layer = Dense(5, activation='sigmoid')(attention)\nmodel = Model(inputs=input_layer, outputs=output_layer)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X, Y, epochs=10, batch_size=100, validation_split=0.2)\n\n# Evaluate the model on test data (replace with your test data)\ntest_data = df_test['Comment'] # Replace with your actual test data\ntest_labels = df_test['Category']  # Replace with your actual test labels\n\ntest_sequences = tokenizer.texts_to_sequences(test_data)\nX_test = pad_sequences(test_sequences, maxlen=max_sequence_length)\nY_test = tf.keras.utils.to_categorical(test_labels, num_classes=5)\n\nloss, accuracy = model.evaluate(X_test, Y_test)\nprint(f'Test Loss: {loss}, Test Accuracy: {accuracy}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-23T02:18:33.971803Z","iopub.execute_input":"2024-01-23T02:18:33.972542Z","iopub.status.idle":"2024-01-23T02:23:04.083365Z","shell.execute_reply.started":"2024-01-23T02:18:33.972502Z","shell.execute_reply":"2024-01-23T02:23:04.082582Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Epoch 1/10\n161/161 [==============================] - 27s 149ms/step - loss: 1.3003 - accuracy: 0.5000 - val_loss: 1.4383 - val_accuracy: 0.0000e+00\nEpoch 2/10\n161/161 [==============================] - 24s 146ms/step - loss: 1.2656 - accuracy: 0.5048 - val_loss: 1.6667 - val_accuracy: 0.0000e+00\nEpoch 3/10\n161/161 [==============================] - 24s 147ms/step - loss: 1.0613 - accuracy: 0.5717 - val_loss: 1.2293 - val_accuracy: 0.3488\nEpoch 4/10\n161/161 [==============================] - 24s 148ms/step - loss: 0.8783 - accuracy: 0.6583 - val_loss: 1.2128 - val_accuracy: 0.4586\nEpoch 5/10\n161/161 [==============================] - 25s 156ms/step - loss: 0.7765 - accuracy: 0.7026 - val_loss: 0.9506 - val_accuracy: 0.6363\nEpoch 6/10\n161/161 [==============================] - 25s 157ms/step - loss: 0.7000 - accuracy: 0.7369 - val_loss: 1.4678 - val_accuracy: 0.4531\nEpoch 7/10\n161/161 [==============================] - 24s 150ms/step - loss: 0.6265 - accuracy: 0.7658 - val_loss: 1.4011 - val_accuracy: 0.5208\nEpoch 8/10\n161/161 [==============================] - 24s 147ms/step - loss: 0.5656 - accuracy: 0.7905 - val_loss: 1.5742 - val_accuracy: 0.4864\nEpoch 9/10\n161/161 [==============================] - 24s 146ms/step - loss: 0.5027 - accuracy: 0.8155 - val_loss: 1.8325 - val_accuracy: 0.4722\nEpoch 10/10\n161/161 [==============================] - 23s 143ms/step - loss: 0.4518 - accuracy: 0.8341 - val_loss: 1.9505 - val_accuracy: 0.4737\n157/157 [==============================] - 3s 20ms/step - loss: 1.6475 - accuracy: 0.5199\nTest Loss: 1.6474559307098389, Test Accuracy: 0.5199123620986938\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\ny_pred = model.predict(X_test)\n\n# Calculate macro recall\nmacro_recall = recall_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\nmacro_precision = precision_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n# Print the results\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n  Macro Recall: {:0.3f}\\n  Macro Precision: {:0.3f}'.format(loss, accuracy, macro_recall, macro_precision))\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-23T02:23:34.638866Z","iopub.execute_input":"2024-01-23T02:23:34.639269Z","iopub.status.idle":"2024-01-23T02:23:40.252428Z","shell.execute_reply.started":"2024-01-23T02:23:34.639238Z","shell.execute_reply":"2024-01-23T02:23:40.251263Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"157/157 [==============================] - 4s 20ms/step\nTest set\n  Loss: 1.647\n  Accuracy: 0.520\n  Macro Recall: 0.367\n  Macro Precision: 0.368\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Attention\nfrom tensorflow.keras.models import Model\n# import tensorflow_addons as tfa\n\n# Sample data and labels (replace with your own dataset)\ndata = df_train['Comment']\nlabels = df_train['Category']\n\n# Tokenize text data\nmax_words = 10000\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(data)\nsequences = tokenizer.texts_to_sequences(data)\nword_index = tokenizer.word_index\nmax_sequence_length = 100  # Adjust this based on your data\n\n# Pad sequences\nX = pad_sequences(sequences, maxlen=max_sequence_length)\n\n# One-hot encode labels\nY = tf.keras.utils.to_categorical(labels, num_classes=5)\n\n# Create LSTM with Attention model\ninput_layer = Input(shape=(max_sequence_length,))\nembedding_layer = Embedding(max_words, 100)(input_layer)\nlstm_layer = LSTM(64, return_sequences=True)(embedding_layer)\n\n# Attention mechanism using TensorFlow Addons\nattention = Attention()([lstm_layer, lstm_layer])\nattention = tf.keras.layers.GlobalAveragePooling1D()(attention)\n\noutput_layer = Dense(5, activation='sigmoid')(attention)\nmodel = Model(inputs=input_layer, outputs=output_layer)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X, Y, epochs=10, batch_size=300, validation_split=0.2)\n\n# Evaluate the model on test data (replace with your test data)\ntest_data = df_test['Comment'] # Replace with your actual test data\ntest_labels = df_test['Category']  # Replace with your actual test labels\n\ntest_sequences = tokenizer.texts_to_sequences(test_data)\nX_test = pad_sequences(test_sequences, maxlen=max_sequence_length)\nY_test = tf.keras.utils.to_categorical(test_labels, num_classes=5)\n\nloss, accuracy = model.evaluate(X_test, Y_test)\nprint(f'Test Loss: {loss}, Test Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-01-23T02:24:34.516350Z","iopub.execute_input":"2024-01-23T02:24:34.516757Z","iopub.status.idle":"2024-01-23T02:27:28.061176Z","shell.execute_reply.started":"2024-01-23T02:24:34.516727Z","shell.execute_reply":"2024-01-23T02:27:28.060102Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch 1/10\n54/54 [==============================] - 20s 323ms/step - loss: 1.3350 - accuracy: 0.4945 - val_loss: 1.4507 - val_accuracy: 0.0000e+00\nEpoch 2/10\n54/54 [==============================] - 17s 312ms/step - loss: 1.2792 - accuracy: 0.5041 - val_loss: 1.5729 - val_accuracy: 0.0000e+00\nEpoch 3/10\n54/54 [==============================] - 17s 309ms/step - loss: 1.2783 - accuracy: 0.5041 - val_loss: 1.5370 - val_accuracy: 0.0000e+00\nEpoch 4/10\n54/54 [==============================] - 16s 306ms/step - loss: 1.2763 - accuracy: 0.5041 - val_loss: 1.4725 - val_accuracy: 0.0000e+00\nEpoch 5/10\n54/54 [==============================] - 16s 303ms/step - loss: 1.2192 - accuracy: 0.5104 - val_loss: 1.2057 - val_accuracy: 0.3346\nEpoch 6/10\n54/54 [==============================] - 16s 302ms/step - loss: 1.0175 - accuracy: 0.5964 - val_loss: 1.3032 - val_accuracy: 0.3092\nEpoch 7/10\n54/54 [==============================] - 17s 308ms/step - loss: 0.8075 - accuracy: 0.6968 - val_loss: 1.0351 - val_accuracy: 0.5753\nEpoch 8/10\n54/54 [==============================] - 17s 307ms/step - loss: 0.6663 - accuracy: 0.7634 - val_loss: 1.3009 - val_accuracy: 0.5133\nEpoch 9/10\n54/54 [==============================] - 16s 306ms/step - loss: 0.5508 - accuracy: 0.8119 - val_loss: 1.3186 - val_accuracy: 0.5517\nEpoch 10/10\n54/54 [==============================] - 16s 306ms/step - loss: 0.4782 - accuracy: 0.8417 - val_loss: 1.3691 - val_accuracy: 0.5621\n157/157 [==============================] - 3s 19ms/step - loss: 1.3339 - accuracy: 0.5526\nTest Loss: 1.3338813781738281, Test Accuracy: 0.5525686740875244\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\ny_pred = model.predict(X_test)\n\n# Calculate macro recall\nmacro_recall = recall_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\nmacro_precision = precision_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n# Print the results\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n  Macro Recall: {:0.3f}\\n  Macro Precision: {:0.3f}'.format(loss, accuracy, macro_recall, macro_precision))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-23T02:27:28.062890Z","iopub.execute_input":"2024-01-23T02:27:28.063208Z","iopub.status.idle":"2024-01-23T02:27:33.625512Z","shell.execute_reply.started":"2024-01-23T02:27:28.063179Z","shell.execute_reply":"2024-01-23T02:27:33.624460Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"157/157 [==============================] - 3s 19ms/step\nTest set\n  Loss: 1.334\n  Accuracy: 0.553\n  Macro Recall: 0.380\n  Macro Precision: 0.391\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Attention\nfrom tensorflow.keras.models import Model\n# import tensorflow_addons as tfa\n\n# Sample data and labels (replace with your own dataset)\ndata = df_train['Comment']\nlabels = df_train['Category']\n\n# Tokenize text data\nmax_words = 10000\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(data)\nsequences = tokenizer.texts_to_sequences(data)\nword_index = tokenizer.word_index\nmax_sequence_length = 100  # Adjust this based on your data\n\n# Pad sequences\nX = pad_sequences(sequences, maxlen=max_sequence_length)\n\n# One-hot encode labels\nY = tf.keras.utils.to_categorical(labels, num_classes=5)\n\n# Create LSTM with Attention model\ninput_layer = Input(shape=(max_sequence_length,))\nembedding_layer = Embedding(max_words, 100)(input_layer)\nlstm_layer = LSTM(64, return_sequences=True)(embedding_layer)\n\n# Attention mechanism using TensorFlow Addons\nattention = Attention()([lstm_layer, lstm_layer])\nattention = tf.keras.layers.GlobalAveragePooling1D()(attention)\n\noutput_layer = Dense(5, activation='sigmoid')(attention)\nmodel = Model(inputs=input_layer, outputs=output_layer)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X, Y, epochs=10, batch_size=100, validation_split=0.2)\n\n# Evaluate the model on test data (replace with your test data)\ntest_data = df_test['Comment'] # Replace with your actual test data\ntest_labels = df_test['Category']  # Replace with your actual test labels\n\ntest_sequences = tokenizer.texts_to_sequences(test_data)\nX_test = pad_sequences(test_sequences, maxlen=max_sequence_length)\nY_test = tf.keras.utils.to_categorical(test_labels, num_classes=5)\n\nloss, accuracy = model.evaluate(X_test, Y_test)\nprint(f'Test Loss: {loss}, Test Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-01-23T02:53:24.794453Z","iopub.execute_input":"2024-01-23T02:53:24.795154Z","iopub.status.idle":"2024-01-23T02:57:54.717321Z","shell.execute_reply.started":"2024-01-23T02:53:24.795113Z","shell.execute_reply":"2024-01-23T02:57:54.716463Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1/10\n161/161 [==============================] - 27s 151ms/step - loss: 1.2990 - accuracy: 0.5025 - val_loss: 1.5037 - val_accuracy: 0.0000e+00\nEpoch 2/10\n161/161 [==============================] - 24s 150ms/step - loss: 1.2682 - accuracy: 0.5046 - val_loss: 1.4110 - val_accuracy: 0.0000e+00\nEpoch 3/10\n161/161 [==============================] - 24s 148ms/step - loss: 1.0380 - accuracy: 0.5822 - val_loss: 1.1848 - val_accuracy: 0.3211\nEpoch 4/10\n161/161 [==============================] - 24s 150ms/step - loss: 0.7901 - accuracy: 0.6998 - val_loss: 1.4222 - val_accuracy: 0.4025\nEpoch 5/10\n161/161 [==============================] - 26s 160ms/step - loss: 0.6291 - accuracy: 0.7770 - val_loss: 1.3225 - val_accuracy: 0.5554\nEpoch 6/10\n161/161 [==============================] - 26s 164ms/step - loss: 0.5159 - accuracy: 0.8232 - val_loss: 1.2142 - val_accuracy: 0.6057\nEpoch 7/10\n161/161 [==============================] - 27s 168ms/step - loss: 0.4297 - accuracy: 0.8545 - val_loss: 1.5526 - val_accuracy: 0.5437\nEpoch 8/10\n161/161 [==============================] - 27s 165ms/step - loss: 0.3648 - accuracy: 0.8754 - val_loss: 2.0914 - val_accuracy: 0.4471\nEpoch 9/10\n161/161 [==============================] - 28s 176ms/step - loss: 0.3052 - accuracy: 0.8962 - val_loss: 2.0565 - val_accuracy: 0.5136\nEpoch 10/10\n161/161 [==============================] - 24s 151ms/step - loss: 0.2626 - accuracy: 0.9122 - val_loss: 2.3636 - val_accuracy: 0.4780\n157/157 [==============================] - 3s 20ms/step - loss: 1.8531 - accuracy: 0.5356\nTest Loss: 1.853079080581665, Test Accuracy: 0.5356431603431702\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\ny_pred = model.predict(X_test)\n\n# Calculate macro recall\nmacro_recall = recall_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\nmacro_precision = precision_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n# Print the results\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n  Macro Recall: {:0.3f}\\n  Macro Precision: {:0.3f}'.format(loss, accuracy, macro_recall, macro_precision))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-23T02:57:54.720948Z","iopub.execute_input":"2024-01-23T02:57:54.721325Z","iopub.status.idle":"2024-01-23T02:58:00.295451Z","shell.execute_reply.started":"2024-01-23T02:57:54.721290Z","shell.execute_reply":"2024-01-23T02:58:00.294239Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"157/157 [==============================] - 4s 20ms/step\nTest set\n  Loss: 1.853\n  Accuracy: 0.536\n  Macro Recall: 0.387\n  Macro Precision: 0.386\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Attention\nfrom tensorflow.keras.models import Model\n# import tensorflow_addons as tfa\n\n# Sample data and labels (replace with your own dataset)\ndata = df_train['Comment']\nlabels = df_train['Category']\n\n# Tokenize text data\nmax_words = 10000\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(data)\nsequences = tokenizer.texts_to_sequences(data)\nword_index = tokenizer.word_index\nmax_sequence_length = 100  # Adjust this based on your data\n\n# Pad sequences\nX = pad_sequences(sequences, maxlen=max_sequence_length)\n\n# One-hot encode labels\nY = tf.keras.utils.to_categorical(labels, num_classes=5)\n\n# Create LSTM with Attention model\ninput_layer = Input(shape=(max_sequence_length,))\nembedding_layer = Embedding(max_words, 100)(input_layer)\nlstm_layer = LSTM(64, return_sequences=True)(embedding_layer)\n\n# Attention mechanism using TensorFlow Addons\nattention = Attention()([lstm_layer, lstm_layer])\nattention = tf.keras.layers.GlobalAveragePooling1D()(attention)\n\noutput_layer = Dense(5, activation='sigmoid')(attention)\nmodel = Model(inputs=input_layer, outputs=output_layer)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X, Y, epochs=10, batch_size=500, validation_split=0.2)\n\n# Evaluate the model on test data (replace with your test data)\ntest_data = df_test['Comment'] # Replace with your actual test data\ntest_labels = df_test['Category']  # Replace with your actual test labels\n\ntest_sequences = tokenizer.texts_to_sequences(test_data)\nX_test = pad_sequences(test_sequences, maxlen=max_sequence_length)\nY_test = tf.keras.utils.to_categorical(test_labels, num_classes=5)\n\nloss, accuracy = model.evaluate(X_test, Y_test)\nprint(f'Test Loss: {loss}, Test Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-01-23T02:58:00.296850Z","iopub.execute_input":"2024-01-23T02:58:00.297177Z","iopub.status.idle":"2024-01-23T03:00:43.820770Z","shell.execute_reply.started":"2024-01-23T02:58:00.297148Z","shell.execute_reply":"2024-01-23T03:00:43.819649Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/10\n33/33 [==============================] - 19s 489ms/step - loss: 1.3583 - accuracy: 0.4959 - val_loss: 1.4784 - val_accuracy: 0.0000e+00\nEpoch 2/10\n33/33 [==============================] - 15s 456ms/step - loss: 1.2795 - accuracy: 0.5041 - val_loss: 1.4536 - val_accuracy: 0.0000e+00\nEpoch 3/10\n33/33 [==============================] - 15s 462ms/step - loss: 1.2788 - accuracy: 0.5041 - val_loss: 1.4352 - val_accuracy: 0.0000e+00\nEpoch 4/10\n33/33 [==============================] - 15s 454ms/step - loss: 1.2780 - accuracy: 0.5041 - val_loss: 1.5235 - val_accuracy: 0.0000e+00\nEpoch 5/10\n33/33 [==============================] - 16s 484ms/step - loss: 1.2759 - accuracy: 0.5041 - val_loss: 1.4773 - val_accuracy: 0.0000e+00\nEpoch 6/10\n33/33 [==============================] - 16s 499ms/step - loss: 1.2646 - accuracy: 0.5041 - val_loss: 1.4943 - val_accuracy: 0.0000e+00\nEpoch 7/10\n33/33 [==============================] - 16s 473ms/step - loss: 1.1893 - accuracy: 0.5114 - val_loss: 1.5322 - val_accuracy: 0.0000e+00\nEpoch 8/10\n33/33 [==============================] - 15s 456ms/step - loss: 1.0071 - accuracy: 0.6005 - val_loss: 1.4877 - val_accuracy: 0.2487\nEpoch 9/10\n33/33 [==============================] - 15s 468ms/step - loss: 0.8099 - accuracy: 0.6922 - val_loss: 1.0884 - val_accuracy: 0.5385\nEpoch 10/10\n33/33 [==============================] - 16s 494ms/step - loss: 0.6661 - accuracy: 0.7608 - val_loss: 1.3909 - val_accuracy: 0.4503\n157/157 [==============================] - 3s 20ms/step - loss: 1.2323 - accuracy: 0.5263\nTest Loss: 1.2322776317596436, Test Accuracy: 0.5262843370437622\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\ny_pred = model.predict(X_test)\n\n# Calculate macro recall\nmacro_recall = recall_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\nmacro_precision = precision_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n# Print the results\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n  Macro Recall: {:0.3f}\\n  Macro Precision: {:0.3f}'.format(loss, accuracy, macro_recall, macro_precision))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-23T03:00:43.825235Z","iopub.execute_input":"2024-01-23T03:00:43.825601Z","iopub.status.idle":"2024-01-23T03:00:47.481856Z","shell.execute_reply.started":"2024-01-23T03:00:43.825569Z","shell.execute_reply":"2024-01-23T03:00:47.480783Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"157/157 [==============================] - 4s 20ms/step\nTest set\n  Loss: 1.232\n  Accuracy: 0.526\n  Macro Recall: 0.364\n  Macro Precision: 0.389\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Attention\nfrom tensorflow.keras.models import Model\n# import tensorflow_addons as tfa\n\n# Sample data and labels (replace with your own dataset)\ndata = df_train['Comment']\nlabels = df_train['Category']\n\n# Tokenize text data\nmax_words = 10000\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(data)\nsequences = tokenizer.texts_to_sequences(data)\nword_index = tokenizer.word_index\nmax_sequence_length = 100  # Adjust this based on your data\n\n# Pad sequences\nX = pad_sequences(sequences, maxlen=max_sequence_length)\n\n# One-hot encode labels\nY = tf.keras.utils.to_categorical(labels, num_classes=5)\n\n# Create LSTM with Attention model\ninput_layer = Input(shape=(max_sequence_length,))\nembedding_layer = Embedding(max_words, 100)(input_layer)\nlstm_layer = LSTM(64, return_sequences=True)(embedding_layer)\n\n# Attention mechanism using TensorFlow Addons\nattention = Attention()([lstm_layer, lstm_layer])\nattention = tf.keras.layers.GlobalAveragePooling1D()(attention)\n\noutput_layer = Dense(5, activation='sigmoid')(attention)\nmodel = Model(inputs=input_layer, outputs=output_layer)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X, Y, epochs=10, batch_size=32, validation_split=0.2)\n\n# Evaluate the model on test data (replace with your test data)\ntest_data = df_test['Comment'] # Replace with your actual test data\ntest_labels = df_test['Category']  # Replace with your actual test labels\n\ntest_sequences = tokenizer.texts_to_sequences(test_data)\nX_test = pad_sequences(test_sequences, maxlen=max_sequence_length)\nY_test = tf.keras.utils.to_categorical(test_labels, num_classes=5)\n\nloss, accuracy = model.evaluate(X_test, Y_test)\nprint(f'Test Loss: {loss}, Test Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-01-23T03:55:37.876480Z","iopub.execute_input":"2024-01-23T03:55:37.877431Z","iopub.status.idle":"2024-01-23T04:02:10.037092Z","shell.execute_reply.started":"2024-01-23T03:55:37.877385Z","shell.execute_reply":"2024-01-23T04:02:10.035771Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Epoch 1/10\n503/503 [==============================] - 41s 75ms/step - loss: 1.2881 - accuracy: 0.5025 - val_loss: 1.5428 - val_accuracy: 0.0000e+00\nEpoch 2/10\n503/503 [==============================] - 40s 80ms/step - loss: 1.0651 - accuracy: 0.5712 - val_loss: 1.4009 - val_accuracy: 0.0876\nEpoch 3/10\n503/503 [==============================] - 38s 75ms/step - loss: 0.7780 - accuracy: 0.7017 - val_loss: 1.1449 - val_accuracy: 0.5671\nEpoch 4/10\n503/503 [==============================] - 37s 74ms/step - loss: 0.5846 - accuracy: 0.7873 - val_loss: 1.3489 - val_accuracy: 0.5345\nEpoch 5/10\n503/503 [==============================] - 38s 76ms/step - loss: 0.4435 - accuracy: 0.8371 - val_loss: 1.4863 - val_accuracy: 0.5738\nEpoch 6/10\n503/503 [==============================] - 38s 75ms/step - loss: 0.3450 - accuracy: 0.8772 - val_loss: 2.0660 - val_accuracy: 0.4680\nEpoch 7/10\n503/503 [==============================] - 38s 75ms/step - loss: 0.2746 - accuracy: 0.9027 - val_loss: 1.9246 - val_accuracy: 0.5407\nEpoch 8/10\n503/503 [==============================] - 38s 75ms/step - loss: 0.2234 - accuracy: 0.9212 - val_loss: 2.2990 - val_accuracy: 0.5151\nEpoch 9/10\n503/503 [==============================] - 38s 76ms/step - loss: 0.1855 - accuracy: 0.9322 - val_loss: 2.8085 - val_accuracy: 0.4576\nEpoch 10/10\n503/503 [==============================] - 38s 76ms/step - loss: 0.1595 - accuracy: 0.9407 - val_loss: 2.6669 - val_accuracy: 0.4939\n157/157 [==============================] - 4s 23ms/step - loss: 2.3435 - accuracy: 0.5313\nTest Loss: 2.343489408493042, Test Accuracy: 0.5312624573707581\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\ny_pred = model.predict(X_test)\n\n# Calculate macro recall\nmacro_recall = recall_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\nmacro_precision = precision_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n# Print the results\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n  Macro Recall: {:0.3f}\\n  Macro Precision: {:0.3f}'.format(loss, accuracy, macro_recall, macro_precision))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-23T04:02:14.015837Z","iopub.execute_input":"2024-01-23T04:02:14.016224Z","iopub.status.idle":"2024-01-23T04:02:17.530924Z","shell.execute_reply.started":"2024-01-23T04:02:14.016193Z","shell.execute_reply":"2024-01-23T04:02:17.529672Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"157/157 [==============================] - 3s 21ms/step\nTest set\n  Loss: 2.343\n  Accuracy: 0.531\n  Macro Recall: 0.377\n  Macro Precision: 0.381\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Attention\nfrom tensorflow.keras.models import Model\n# import tensorflow_addons as tfa\n\n# Sample data and labels (replace with your own dataset)\ndata = df_train['Comment']\nlabels = df_train['Category']\n\n# Tokenize text data\nmax_words = 10000\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(data)\nsequences = tokenizer.texts_to_sequences(data)\nword_index = tokenizer.word_index\nmax_sequence_length = 100  # Adjust this based on your data\n\n# Pad sequences\nX = pad_sequences(sequences, maxlen=max_sequence_length)\n\n# One-hot encode labels\nY = tf.keras.utils.to_categorical(labels, num_classes=5)\n\n# Create LSTM with Attention model\ninput_layer = Input(shape=(max_sequence_length,))\nembedding_layer = Embedding(max_words, 100)(input_layer)\nlstm_layer = LSTM(64, return_sequences=True)(embedding_layer)\n\n# Attention mechanism using TensorFlow Addons\nattention = Attention()([lstm_layer, lstm_layer])\nattention = tf.keras.layers.GlobalAveragePooling1D()(attention)\n\noutput_layer = Dense(5, activation='sigmoid')(attention)\nmodel = Model(inputs=input_layer, outputs=output_layer)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X, Y, epochs=10, batch_size=300, validation_split=0.2)\n\n# Evaluate the model on test data (replace with your test data)\ntest_data = df_test['Comment'] # Replace with your actual test data\ntest_labels = df_test['Category']  # Replace with your actual test labels\n\ntest_sequences = tokenizer.texts_to_sequences(test_data)\nX_test = pad_sequences(test_sequences, maxlen=max_sequence_length)\nY_test = tf.keras.utils.to_categorical(test_labels, num_classes=5)\n\nloss, accuracy = model.evaluate(X_test, Y_test)\nprint(f'Test Loss: {loss}, Test Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-01-23T04:20:15.375397Z","iopub.execute_input":"2024-01-23T04:20:15.375954Z","iopub.status.idle":"2024-01-23T04:23:17.982369Z","shell.execute_reply.started":"2024-01-23T04:20:15.375913Z","shell.execute_reply":"2024-01-23T04:23:17.981061Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Epoch 1/10\n54/54 [==============================] - 21s 344ms/step - loss: 1.3160 - accuracy: 0.3915 - val_loss: 1.2574 - val_accuracy: 0.3869\nEpoch 2/10\n54/54 [==============================] - 17s 322ms/step - loss: 1.2610 - accuracy: 0.3976 - val_loss: 1.2539 - val_accuracy: 0.3998\nEpoch 3/10\n54/54 [==============================] - 18s 327ms/step - loss: 1.2585 - accuracy: 0.4058 - val_loss: 1.2518 - val_accuracy: 0.3998\nEpoch 4/10\n54/54 [==============================] - 17s 319ms/step - loss: 1.2446 - accuracy: 0.4318 - val_loss: 1.2055 - val_accuracy: 0.4730\nEpoch 5/10\n54/54 [==============================] - 18s 325ms/step - loss: 1.0881 - accuracy: 0.5409 - val_loss: 1.0689 - val_accuracy: 0.5479\nEpoch 6/10\n54/54 [==============================] - 17s 322ms/step - loss: 0.8550 - accuracy: 0.6734 - val_loss: 1.0443 - val_accuracy: 0.5763\nEpoch 7/10\n54/54 [==============================] - 17s 323ms/step - loss: 0.6898 - accuracy: 0.7539 - val_loss: 1.0661 - val_accuracy: 0.5910\nEpoch 8/10\n54/54 [==============================] - 17s 319ms/step - loss: 0.5860 - accuracy: 0.8030 - val_loss: 1.1843 - val_accuracy: 0.5843\nEpoch 9/10\n54/54 [==============================] - 17s 319ms/step - loss: 0.4920 - accuracy: 0.8445 - val_loss: 1.1974 - val_accuracy: 0.5795\nEpoch 10/10\n54/54 [==============================] - 17s 314ms/step - loss: 0.4298 - accuracy: 0.8662 - val_loss: 1.2654 - val_accuracy: 0.5887\n157/157 [==============================] - 3s 21ms/step - loss: 1.3098 - accuracy: 0.5659\nTest Loss: 1.3097609281539917, Test Accuracy: 0.5659099817276001\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\ny_pred = model.predict(X_test)\n\n# Calculate macro recall\nmacro_recall = recall_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\nmacro_precision = precision_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n# Print the results\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n  Macro Recall: {:0.3f}\\n  Macro Precision: {:0.3f}'.format(loss, accuracy, macro_recall, macro_precision))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-23T04:23:17.984185Z","iopub.execute_input":"2024-01-23T04:23:17.985072Z","iopub.status.idle":"2024-01-23T04:23:23.573965Z","shell.execute_reply.started":"2024-01-23T04:23:17.985036Z","shell.execute_reply":"2024-01-23T04:23:23.572721Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"157/157 [==============================] - 4s 20ms/step\nTest set\n  Loss: 1.310\n  Accuracy: 0.566\n  Macro Recall: 0.379\n  Macro Precision: 0.393\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Attention\nfrom tensorflow.keras.models import Model\n# import tensorflow_addons as tfa\n\n# Sample data and labels (replace with your own dataset)\ndata = df_train['Comment']\nlabels = df_train['Category']\n\n# Tokenize text data\nmax_words = 10000\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(data)\nsequences = tokenizer.texts_to_sequences(data)\nword_index = tokenizer.word_index\nmax_sequence_length = 100  # Adjust this based on your data\n\n# Pad sequences\nX = pad_sequences(sequences, maxlen=max_sequence_length)\n\n# One-hot encode labels\nY = tf.keras.utils.to_categorical(labels, num_classes=5)\n\n# Create LSTM with Attention model\ninput_layer = Input(shape=(max_sequence_length,))\nembedding_layer = Embedding(max_words, 100)(input_layer)\nlstm_layer = LSTM(64, return_sequences=True)(embedding_layer)\n\n# Attention mechanism using TensorFlow Addons\nattention = Attention()([lstm_layer, lstm_layer])\nattention = tf.keras.layers.GlobalAveragePooling1D()(attention)\n\noutput_layer = Dense(5, activation='sigmoid')(attention)\nmodel = Model(inputs=input_layer, outputs=output_layer)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X, Y, epochs=10, batch_size=100, validation_split=0.2)\n\n# Evaluate the model on test data (replace with your test data)\ntest_data = df_test['Comment'] # Replace with your actual test data\ntest_labels = df_test['Category']  # Replace with your actual test labels\n\ntest_sequences = tokenizer.texts_to_sequences(test_data)\nX_test = pad_sequences(test_sequences, maxlen=max_sequence_length)\nY_test = tf.keras.utils.to_categorical(test_labels, num_classes=5)\n\nloss, accuracy = model.evaluate(X_test, Y_test)\nprint(f'Test Loss: {loss}, Test Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-01-23T04:23:59.405558Z","iopub.execute_input":"2024-01-23T04:23:59.406149Z","iopub.status.idle":"2024-01-23T04:28:15.566243Z","shell.execute_reply.started":"2024-01-23T04:23:59.406107Z","shell.execute_reply":"2024-01-23T04:28:15.564817Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/10\n161/161 [==============================] - 28s 160ms/step - loss: 1.2844 - accuracy: 0.3931 - val_loss: 1.2578 - val_accuracy: 0.3998\nEpoch 2/10\n161/161 [==============================] - 24s 151ms/step - loss: 1.2578 - accuracy: 0.4063 - val_loss: 1.2336 - val_accuracy: 0.3998\nEpoch 3/10\n161/161 [==============================] - 25s 153ms/step - loss: 1.0764 - accuracy: 0.5454 - val_loss: 1.0233 - val_accuracy: 0.5663\nEpoch 4/10\n161/161 [==============================] - 24s 152ms/step - loss: 0.8017 - accuracy: 0.6990 - val_loss: 1.0228 - val_accuracy: 0.5960\nEpoch 5/10\n161/161 [==============================] - 25s 157ms/step - loss: 0.6353 - accuracy: 0.7746 - val_loss: 1.0625 - val_accuracy: 0.6042\nEpoch 6/10\n161/161 [==============================] - 25s 153ms/step - loss: 0.5116 - accuracy: 0.8266 - val_loss: 1.1897 - val_accuracy: 0.6002\nEpoch 7/10\n161/161 [==============================] - 25s 154ms/step - loss: 0.4175 - accuracy: 0.8595 - val_loss: 1.2760 - val_accuracy: 0.5870\nEpoch 8/10\n161/161 [==============================] - 25s 153ms/step - loss: 0.3359 - accuracy: 0.8913 - val_loss: 1.4316 - val_accuracy: 0.5765\nEpoch 9/10\n161/161 [==============================] - 25s 154ms/step - loss: 0.2826 - accuracy: 0.9098 - val_loss: 1.5727 - val_accuracy: 0.5666\nEpoch 10/10\n161/161 [==============================] - 25s 154ms/step - loss: 0.2392 - accuracy: 0.9233 - val_loss: 1.7211 - val_accuracy: 0.5666\n157/157 [==============================] - 3s 22ms/step - loss: 1.7406 - accuracy: 0.5554\nTest Loss: 1.740644097328186, Test Accuracy: 0.5553564429283142\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\ny_pred = model.predict(X_test)\n\n# Calculate macro recall\nmacro_recall = recall_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\nmacro_precision = precision_score(Y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n# Print the results\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n  Macro Recall: {:0.3f}\\n  Macro Precision: {:0.3f}'.format(loss, accuracy, macro_recall, macro_precision))","metadata":{"execution":{"iopub.status.busy":"2024-01-23T04:28:24.871927Z","iopub.execute_input":"2024-01-23T04:28:24.872341Z","iopub.status.idle":"2024-01-23T04:28:28.802066Z","shell.execute_reply.started":"2024-01-23T04:28:24.872310Z","shell.execute_reply":"2024-01-23T04:28:28.801100Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"157/157 [==============================] - 4s 21ms/step\nTest set\n  Loss: 1.741\n  Accuracy: 0.555\n  Macro Recall: 0.386\n  Macro Precision: 0.391\n","output_type":"stream"}]}]}