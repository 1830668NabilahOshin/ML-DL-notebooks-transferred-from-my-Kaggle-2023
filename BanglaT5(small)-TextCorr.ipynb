{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7449173,"sourceType":"datasetVersion","datasetId":4336043},{"sourceId":1382412,"sourceType":"datasetVersion","datasetId":806606}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets tqdm pandas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-26T21:33:26.644829Z","iopub.execute_input":"2024-01-26T21:33:26.645404Z","iopub.status.idle":"2024-01-26T21:33:41.418710Z","shell.execute_reply.started":"2024-01-26T21:33:26.645366Z","shell.execute_reply":"2024-01-26T21:33:41.417302Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.12.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:33:41.420453Z","iopub.execute_input":"2024-01-26T21:33:41.420918Z","iopub.status.idle":"2024-01-26T21:33:55.046662Z","shell.execute_reply.started":"2024-01-26T21:33:41.420851Z","shell.execute_reply":"2024-01-26T21:33:55.045390Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:33:55.050425Z","iopub.execute_input":"2024-01-26T21:33:55.050905Z","iopub.status.idle":"2024-01-26T21:34:08.793031Z","shell.execute_reply.started":"2024-01-26T21:33:55.050849Z","shell.execute_reply":"2024-01-26T21:34:08.791612Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:34:08.794504Z","iopub.execute_input":"2024-01-26T21:34:08.794854Z","iopub.status.idle":"2024-01-26T21:34:10.323475Z","shell.execute_reply.started":"2024-01-26T21:34:08.794819Z","shell.execute_reply":"2024-01-26T21:34:10.322292Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\nUsage:   \n  pip <command> [options]\n\nCommands:\n  install                     Install packages.\n  download                    Download packages.\n  uninstall                   Uninstall packages.\n  freeze                      Output installed packages in requirements format.\n  inspect                     Inspect the python environment.\n  list                        List installed packages.\n  show                        Show information about installed packages.\n  check                       Verify installed packages have compatible dependencies.\n  config                      Manage local and global configuration.\n  search                      Search PyPI for packages.\n  cache                       Inspect and manage pip's wheel cache.\n  index                       Inspect information available from package indexes.\n  wheel                       Build wheels from your requirements.\n  hash                        Compute hashes of package archives.\n  completion                  A helper command used for command completion.\n  debug                       Show information useful for debugging.\n  help                        Show help for commands.\n\nGeneral Options:\n  -h, --help                  Show help.\n  --debug                     Let unhandled exceptions propagate outside the\n                              main subroutine, instead of logging them to\n                              stderr.\n  --isolated                  Run pip in an isolated mode, ignoring\n                              environment variables and user configuration.\n  --require-virtualenv        Allow pip to only run in a virtual environment;\n                              exit with an error otherwise.\n  --python <python>           Run pip with the specified Python interpreter.\n  -v, --verbose               Give more output. Option is additive, and can be\n                              used up to 3 times.\n  -V, --version               Show version and exit.\n  -q, --quiet                 Give less output. Option is additive, and can be\n                              used up to 3 times (corresponding to WARNING,\n                              ERROR, and CRITICAL logging levels).\n  --log <path>                Path to a verbose appending log.\n  --no-input                  Disable prompting for input.\n  --keyring-provider <keyring_provider>\n                              Enable the credential lookup via the keyring\n                              library if user input is allowed. Specify which\n                              mechanism to use [disabled, import, subprocess].\n                              (default: disabled)\n  --proxy <proxy>             Specify a proxy in the form\n                              scheme://[user:passwd@]proxy.server:port.\n  --retries <retries>         Maximum number of retries each connection should\n                              attempt (default 5 times).\n  --timeout <sec>             Set the socket timeout (default 15 seconds).\n  --exists-action <action>    Default action when a path already exists:\n                              (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.\n  --trusted-host <hostname>   Mark this host or host:port pair as trusted,\n                              even though it does not have valid or any HTTPS.\n  --cert <path>               Path to PEM-encoded CA certificate bundle. If\n                              provided, overrides the default. See 'SSL\n                              Certificate Verification' in pip documentation\n                              for more information.\n  --client-cert <path>        Path to SSL client certificate, a single file\n                              containing the private key and the certificate\n                              in PEM format.\n  --cache-dir <dir>           Store the cache data in <dir>.\n  --no-cache-dir              Disable the cache.\n  --disable-pip-version-check\n                              Don't periodically check PyPI to determine\n                              whether a new version of pip is available for\n                              download. Implied with --no-index.\n  --no-color                  Suppress colored output.\n  --no-python-version-warning\n                              Silence deprecation warnings for upcoming\n                              unsupported Pythons.\n  --use-feature <feature>     Enable new functionality, that may be backward\n                              incompatible.\n  --use-deprecated <feature>  Enable deprecated functionality, that will be\n                              removed in the future.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:34:10.325126Z","iopub.execute_input":"2024-01-26T21:34:10.325497Z","iopub.status.idle":"2024-01-26T21:34:10.816251Z","shell.execute_reply.started":"2024-01-26T21:34:10.325460Z","shell.execute_reply":"2024-01-26T21:34:10.815367Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:34:10.817573Z","iopub.execute_input":"2024-01-26T21:34:10.818419Z","iopub.status.idle":"2024-01-26T21:34:11.990788Z","shell.execute_reply.started":"2024-01-26T21:34:10.818378Z","shell.execute_reply":"2024-01-26T21:34:11.989656Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Fri Jan 26 21:34:11 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   41C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   43C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import argparse\nimport glob\nimport os\nimport json\nimport time\nimport logging\nimport random\nimport re\nfrom itertools import chain\nfrom string import punctuation\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (\n    AdamW,\n    T5ForConditionalGeneration,\n    AutoTokenizer,\n    get_linear_schedule_with_warmup\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:34:11.992602Z","iopub.execute_input":"2024-01-26T21:34:11.993472Z","iopub.status.idle":"2024-01-26T21:34:12.002148Z","shell.execute_reply.started":"2024-01-26T21:34:11.993426Z","shell.execute_reply":"2024-01-26T21:34:12.001076Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\nimport numpy as np\nimport torch\nimport datasets\n\ndef set_seed(seed):\n  random.seed(seed)\n  np.random.seed(seed)\n  torch.manual_seed(seed)\n\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:34:12.003593Z","iopub.execute_input":"2024-01-26T21:34:12.004100Z","iopub.status.idle":"2024-01-26T21:34:12.015964Z","shell.execute_reply.started":"2024-01-26T21:34:12.004039Z","shell.execute_reply":"2024-01-26T21:34:12.014867Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:34:12.019585Z","iopub.execute_input":"2024-01-26T21:34:12.019935Z","iopub.status.idle":"2024-01-26T21:34:12.025117Z","shell.execute_reply.started":"2024-01-26T21:34:12.019882Z","shell.execute_reply":"2024-01-26T21:34:12.023943Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/80-20ratiofinal/train8020.csv')\ndf.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:34:12.026352Z","iopub.execute_input":"2024-01-26T21:34:12.026669Z","iopub.status.idle":"2024-01-26T21:34:12.429138Z","shell.execute_reply.started":"2024-01-26T21:34:12.026636Z","shell.execute_reply":"2024-01-26T21:34:12.428099Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(20084, 10)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import (\n    T5ForConditionalGeneration, AutoTokenizer,\n    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n  )\n\nfrom torch.utils.data import Dataset, DataLoader\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:34:12.430463Z","iopub.execute_input":"2024-01-26T21:34:12.430793Z","iopub.status.idle":"2024-01-26T21:34:23.166472Z","shell.execute_reply.started":"2024-01-26T21:34:12.430765Z","shell.execute_reply":"2024-01-26T21:34:23.165551Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def calc_token_len(example):\n    return len(tokenizer(example).input_ids)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:34:23.167909Z","iopub.execute_input":"2024-01-26T21:34:23.170276Z","iopub.status.idle":"2024-01-26T21:34:23.177031Z","shell.execute_reply.started":"2024-01-26T21:34:23.170240Z","shell.execute_reply":"2024-01-26T21:34:23.175689Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/80-20ratiofinal/train8020.csv')\ntest_df = pd.read_csv('/kaggle/input/80-20ratiofinal/test8020.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:34:23.571058Z","iopub.execute_input":"2024-01-26T21:34:23.571509Z","iopub.status.idle":"2024-01-26T21:34:23.917658Z","shell.execute_reply.started":"2024-01-26T21:34:23.571464Z","shell.execute_reply":"2024-01-26T21:34:23.916482Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model_name = 'csebuetnlp/banglat5_small'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:34:23.919118Z","iopub.execute_input":"2024-01-26T21:34:23.919451Z","iopub.status.idle":"2024-01-26T21:34:30.143723Z","shell.execute_reply.started":"2024-01-26T21:34:23.919422Z","shell.execute_reply":"2024-01-26T21:34:30.142694Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.83k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baffc5006a8344f49593f49cebbf3ee2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/646 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1261c34cb3de41c2aca7779378047d81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be07e559ccba4149920a5306c223e91d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c576abffad224f3397acab73d175ce82"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be0c437527a7429990fdf085bb5a032d"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install git+https://github.com/csebuetnlp/normalizer","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:34:30.147610Z","iopub.execute_input":"2024-01-26T21:34:30.148123Z","iopub.status.idle":"2024-01-26T21:34:52.898143Z","shell.execute_reply.started":"2024-01-26T21:34:30.148090Z","shell.execute_reply":"2024-01-26T21:34:52.896866Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/csebuetnlp/normalizer\n  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-rqqs20fl\n  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-rqqs20fl\n  Resolved https://github.com/csebuetnlp/normalizer to commit d405944dde5ceeacb7c2fd3245ae2a9dea5f35c9\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from normalizer==0.0.1) (2023.8.8)\nCollecting emoji==1.4.2 (from normalizer==0.0.1)\n  Downloading emoji-1.4.2.tar.gz (184 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy==6.0.3 (from normalizer==0.0.1)\n  Downloading ftfy-6.0.3.tar.gz (64 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy==6.0.3->normalizer==0.0.1) (0.2.6)\nBuilding wheels for collected packages: normalizer, emoji, ftfy\n  Building wheel for normalizer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for normalizer: filename=normalizer-0.0.1-py3-none-any.whl size=6857 sha256=9ecfddab185a7168e73a9b9f4fd02e88b55a3f3f859a448053aada812c61a2c2\n  Stored in directory: /tmp/pip-ephem-wheel-cache-vib6l6w2/wheels/2e/79/9c/cd96d490298305d51d2da11484bb2c25fd1f759a6906708282\n  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186459 sha256=46da4e3697b60f3a31f03b9f34128cf9d185b37d1bee3909a579192c7a001c9b\n  Stored in directory: /root/.cache/pip/wheels/10/f0/fd/4813b1177405693e8da9cdea839f0fb64fde161380e058c827\n  Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41930 sha256=66e126b1739ce04fed31ceeb81c3dab88a85f095fdae30ae1e34a27ee2c274d9\n  Stored in directory: /root/.cache/pip/wheels/92/8e/16/c1e4d4d65685d71085e4e27b44d6ed880b0559474c9ee4ff66\nSuccessfully built normalizer emoji ftfy\nInstalling collected packages: emoji, ftfy, normalizer\n  Attempting uninstall: emoji\n    Found existing installation: emoji 2.9.0\n    Uninstalling emoji-2.9.0:\n      Successfully uninstalled emoji-2.9.0\nSuccessfully installed emoji-1.4.2 ftfy-6.0.3 normalizer-0.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from normalizer import normalize\ntrain_df['Comment'] = train_df['Comment'].apply(normalize)\ntrain_df['Correct Form'] = train_df['Correct Form'].apply(normalize)\ntest_df['Comment'] = test_df['Comment'].apply(normalize)\ntest_df['Correct Form'] = test_df['Correct Form'].apply(normalize)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:34:52.899787Z","iopub.execute_input":"2024-01-26T21:34:52.900185Z","iopub.status.idle":"2024-01-26T21:35:05.769510Z","shell.execute_reply.started":"2024-01-26T21:34:52.900147Z","shell.execute_reply":"2024-01-26T21:35:05.768552Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:35:05.770764Z","iopub.execute_input":"2024-01-26T21:35:05.771109Z","iopub.status.idle":"2024-01-26T21:35:05.778461Z","shell.execute_reply.started":"2024-01-26T21:35:05.771079Z","shell.execute_reply":"2024-01-26T21:35:05.777311Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"((20084, 10), (5022, 10))"},"metadata":{}}]},{"cell_type":"code","source":"def calc_token_len(example):\n    return len(tokenizer(example).input_ids)\n\ntrain_df['input_token_len'] = train_df['Comment'].apply(calc_token_len)\ntest_df['input_token_len'] = test_df['Comment'].apply(calc_token_len)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:35:05.779857Z","iopub.execute_input":"2024-01-26T21:35:05.780232Z","iopub.status.idle":"2024-01-26T21:35:09.701480Z","shell.execute_reply.started":"2024-01-26T21:35:05.780203Z","shell.execute_reply":"2024-01-26T21:35:09.700344Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"      Video ID        Channel name     Time of Publishing  \\\n0  62d6PfZxky0  Bangla Bhuter Golpo  2023-01-16T13:00:48Z   \n1  xgMi3vUaw5Y              Baseera  2022-04-02T19:03:14Z   \n2  8Q7mPmy9eTI  BANGLADESHI REACTOR  2023-01-22T07:00:02Z   \n3  gMeeozaMmIM         Laser Vision  2018-01-13T11:04:17Z   \n4  Vhj-_avEjs0         Ritu Hossain  2023-06-26T12:00:08Z   \n\n                                                                                                 Title  \\\n0  Bhuter Cartoon - Railway Station at 2am Night (True Story) Train Horror Story | Bangla Bhuter Golpo   \n1                                                               সিরাহ ৭ – অ্যাবিসিনিয়া | Bangla Seerah   \n2       লাইভে খেজুর পাতার জা*ইঙ্গা বিক্রি করছে টনি আপা | Salman Muqtadir female version | দাদি দাদা কই   \n3   Surja Dighal Bari | Bangla Movie | Dolly Anwar | Zahirul Haque | Rowshan Jamil | Sheikh Niamat Ali   \n4               রিতু নিজের টাকায় রুম ডেকোরেশন করলো | My Home Tour VLOG | Ritu Hossain | Rakib Hossain   \n\n            Genre  \\\n0  Entertainment    \n1   Miscellaneous   \n2   Entertainment   \n3   Entertainment   \n4   Entertainment   \n\n                                                                                                                                                                                                                                     Comment  \\\n0                                                                                                                                                                      কথা হচ্ছে ওরা এতো ট্রেন পেল কোথায় 🙄 যে এক ঘন্টা পর পর ট্রেন আসতেছে 😳   \n1                                                                                          প্রিয় ভাই আমার,, আপনি আর বেশি বেশি ভিডিও ছাড়েন,, যেন সুনে আমরা আরো বেশি উপকৃর্ত হতে পারি,,, অনেক অপেক্ষায় থাকি।। আল্লাহ জন্য ভালোবাসি আপনাকে।।   \n2                                                                                                                     আপু তোমাকে ধন্যবাদ এইভাবে সবাইকে রিপ্লাই দিয়ে শিক্ষা দেওয়ার জন্য,, তুমি যদি সুখে থাকতে পারো তাহলে ওদের চুলকায় কেনো।   \n3  এখানে আমার মনে হয় কেউ অভিনয় করে নাই। এটা একদম জীবন এর সাথে মিলে গেছে। একদম নিখুঁত অভিনয়। আগের দিন এর ছবি, নাটক দেখতে সত্যি অনেক সুন্দর লাগে অভিনয় যেন নয় যেন আসল শুধু ক্যামেরা বন্দী করা হয়ছে জীবনটাকে 🙂🙂।খুব সুন্দর লাগল ছবি টা 🙂🙂   \n4                                                                                                                                                                                        ওওআপি আমি তোমার রোম দেখার জন্য অনেক অনেক অনেক হেপি😊   \n\n   Error         Category  \\\n0      1  Multiple Errors   \n1      1         Spelling   \n2      1   Code Switching   \n3      1         Spelling   \n4      1         Spelling   \n\n                                                                                                                                                                                                                                 Correct Form  \\\n0                                                                                                                                                                          কথা হচ্ছে ওরা এত ট্রেন পেল কোথায় 🙄 যে এক ঘণ্টা পর পর ট্রেন আসছে 😳   \n1                                                                                             প্রিয় ভাই আমার,, আপনি আর বেশি বেশি ভিডিও ছাড়েন,, যেন শুনে আমরা আরো বেশি উপকৃত হতে পারি,,, অনেক অপেক্ষায় থাকি।। আল্লাহ জন্য ভালোবাসি আপনাকে।।   \n2                                                                                                                         আপু তোমাকে ধন্যবাদ এইভাবে সবাইকে জবাব দিয়ে শিক্ষা দেওয়ার জন্য,, তুমি যদি সুখে থাকতে পারো তাহলে ওদের চুলকায় কেনো।   \n3  এখানে আমার মনে হয় কেউ অভিনয় করে নাই। এটা একদম জীবন এর সাথে মিলে গেছে। একদম নিখুঁত অভিনয়। আগের দিন এর ছবি, নাটক দেখতে সত্যি অনেক সুন্দর লাগে অভিনয় যেন নয় যেন আসল শুধু ক্যামেরা বন্দী করা হয়েছে জীবনটাকে 🙂🙂।খুব সুন্দর লাগল ছবি টা 🙂🙂   \n4                                                                                                                                                                                          আপু আমি তোমার কক্ষ দেখার জন্য অনেক অনেক অনেক খুশি😊   \n\n   Unnamed: 9  input_token_len  \n0         NaN               20  \n1         NaN               37  \n2         NaN               24  \n3         NaN               55  \n4         NaN               16  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Video ID</th>\n      <th>Channel name</th>\n      <th>Time of Publishing</th>\n      <th>Title</th>\n      <th>Genre</th>\n      <th>Comment</th>\n      <th>Error</th>\n      <th>Category</th>\n      <th>Correct Form</th>\n      <th>Unnamed: 9</th>\n      <th>input_token_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>62d6PfZxky0</td>\n      <td>Bangla Bhuter Golpo</td>\n      <td>2023-01-16T13:00:48Z</td>\n      <td>Bhuter Cartoon - Railway Station at 2am Night (True Story) Train Horror Story | Bangla Bhuter Golpo</td>\n      <td>Entertainment</td>\n      <td>কথা হচ্ছে ওরা এতো ট্রেন পেল কোথায় 🙄 যে এক ঘন্টা পর পর ট্রেন আসতেছে 😳</td>\n      <td>1</td>\n      <td>Multiple Errors</td>\n      <td>কথা হচ্ছে ওরা এত ট্রেন পেল কোথায় 🙄 যে এক ঘণ্টা পর পর ট্রেন আসছে 😳</td>\n      <td>NaN</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>xgMi3vUaw5Y</td>\n      <td>Baseera</td>\n      <td>2022-04-02T19:03:14Z</td>\n      <td>সিরাহ ৭ – অ্যাবিসিনিয়া | Bangla Seerah</td>\n      <td>Miscellaneous</td>\n      <td>প্রিয় ভাই আমার,, আপনি আর বেশি বেশি ভিডিও ছাড়েন,, যেন সুনে আমরা আরো বেশি উপকৃর্ত হতে পারি,,, অনেক অপেক্ষায় থাকি।। আল্লাহ জন্য ভালোবাসি আপনাকে।।</td>\n      <td>1</td>\n      <td>Spelling</td>\n      <td>প্রিয় ভাই আমার,, আপনি আর বেশি বেশি ভিডিও ছাড়েন,, যেন শুনে আমরা আরো বেশি উপকৃত হতে পারি,,, অনেক অপেক্ষায় থাকি।। আল্লাহ জন্য ভালোবাসি আপনাকে।।</td>\n      <td>NaN</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8Q7mPmy9eTI</td>\n      <td>BANGLADESHI REACTOR</td>\n      <td>2023-01-22T07:00:02Z</td>\n      <td>লাইভে খেজুর পাতার জা*ইঙ্গা বিক্রি করছে টনি আপা | Salman Muqtadir female version | দাদি দাদা কই</td>\n      <td>Entertainment</td>\n      <td>আপু তোমাকে ধন্যবাদ এইভাবে সবাইকে রিপ্লাই দিয়ে শিক্ষা দেওয়ার জন্য,, তুমি যদি সুখে থাকতে পারো তাহলে ওদের চুলকায় কেনো।</td>\n      <td>1</td>\n      <td>Code Switching</td>\n      <td>আপু তোমাকে ধন্যবাদ এইভাবে সবাইকে জবাব দিয়ে শিক্ষা দেওয়ার জন্য,, তুমি যদি সুখে থাকতে পারো তাহলে ওদের চুলকায় কেনো।</td>\n      <td>NaN</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gMeeozaMmIM</td>\n      <td>Laser Vision</td>\n      <td>2018-01-13T11:04:17Z</td>\n      <td>Surja Dighal Bari | Bangla Movie | Dolly Anwar | Zahirul Haque | Rowshan Jamil | Sheikh Niamat Ali</td>\n      <td>Entertainment</td>\n      <td>এখানে আমার মনে হয় কেউ অভিনয় করে নাই। এটা একদম জীবন এর সাথে মিলে গেছে। একদম নিখুঁত অভিনয়। আগের দিন এর ছবি, নাটক দেখতে সত্যি অনেক সুন্দর লাগে অভিনয় যেন নয় যেন আসল শুধু ক্যামেরা বন্দী করা হয়ছে জীবনটাকে 🙂🙂।খুব সুন্দর লাগল ছবি টা 🙂🙂</td>\n      <td>1</td>\n      <td>Spelling</td>\n      <td>এখানে আমার মনে হয় কেউ অভিনয় করে নাই। এটা একদম জীবন এর সাথে মিলে গেছে। একদম নিখুঁত অভিনয়। আগের দিন এর ছবি, নাটক দেখতে সত্যি অনেক সুন্দর লাগে অভিনয় যেন নয় যেন আসল শুধু ক্যামেরা বন্দী করা হয়েছে জীবনটাকে 🙂🙂।খুব সুন্দর লাগল ছবি টা 🙂🙂</td>\n      <td>NaN</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Vhj-_avEjs0</td>\n      <td>Ritu Hossain</td>\n      <td>2023-06-26T12:00:08Z</td>\n      <td>রিতু নিজের টাকায় রুম ডেকোরেশন করলো | My Home Tour VLOG | Ritu Hossain | Rakib Hossain</td>\n      <td>Entertainment</td>\n      <td>ওওআপি আমি তোমার রোম দেখার জন্য অনেক অনেক অনেক হেপি😊</td>\n      <td>1</td>\n      <td>Spelling</td>\n      <td>আপু আমি তোমার কক্ষ দেখার জন্য অনেক অনেক অনেক খুশি😊</td>\n      <td>NaN</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\ntest_dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:35:09.703192Z","iopub.execute_input":"2024-01-26T21:35:09.703662Z","iopub.status.idle":"2024-01-26T21:35:09.793804Z","shell.execute_reply.started":"2024-01-26T21:35:09.703607Z","shell.execute_reply":"2024-01-26T21:35:09.792747Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Video ID', 'Channel name ', 'Time of Publishing', 'Title', 'Genre', 'Comment', 'Error', 'Category', 'Correct Form', 'Unnamed: 9', 'input_token_len'],\n    num_rows: 5022\n})"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nclass GrammarDataset(Dataset):\n    def __init__(self, dataset, tokenizer,print_text=False):\n        self.dataset = dataset\n        self.pad_to_max_length = False\n        self.tokenizer = tokenizer\n        self.print_text = print_text\n        self.max_len = 64\n\n    def __len__(self):\n        return len(self.dataset)\n\n\n    def tokenize_data(self, example):\n        input_, target_ = example['Comment'], example['Correct Form']\n\n        # tokenize inputs\n        tokenized_inputs = tokenizer(input_, pad_to_max_length=self.pad_to_max_length,\n                                            max_length=self.max_len,\n                                            return_attention_mask=True)\n\n        tokenized_targets = tokenizer(target_, pad_to_max_length=self.pad_to_max_length,\n                                            max_length=self.max_len,\n                                            return_attention_mask=True)\n\n        inputs={\"input_ids\": tokenized_inputs['input_ids'],\n            \"attention_mask\": tokenized_inputs['attention_mask'],\n            \"labels\": tokenized_targets['input_ids']\n        }\n\n        return inputs\n\n\n    def __getitem__(self, index):\n        inputs = self.tokenize_data(self.dataset[index])\n\n        if self.print_text:\n            for k in inputs.keys():\n                print(k, len(inputs[k]))\n\n        return inputs\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:35:39.733347Z","iopub.execute_input":"2024-01-26T21:35:39.733775Z","iopub.status.idle":"2024-01-26T21:35:39.744132Z","shell.execute_reply.started":"2024-01-26T21:35:39.733740Z","shell.execute_reply":"2024-01-26T21:35:39.743059Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"dataset = GrammarDataset(test_dataset, tokenizer, True)\nprint(dataset[121])","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:35:43.672794Z","iopub.execute_input":"2024-01-26T21:35:43.673941Z","iopub.status.idle":"2024-01-26T21:35:43.684250Z","shell.execute_reply.started":"2024-01-26T21:35:43.673879Z","shell.execute_reply":"2024-01-26T21:35:43.683215Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"input_ids 24\nattention_mask 24\nlabels 24\n{'input_ids': [290, 406, 79, 14, 541, 38, 4576, 2082, 1068, 5, 6673, 2008, 17, 30, 881, 840, 11777, 756, 376, 520, 3157, 20, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [290, 406, 79, 14, 541, 38, 4576, 2082, 1068, 5, 158, 2630, 16332, 30, 881, 840, 11777, 756, 376, 520, 3157, 20, 2, 1]}\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:35:46.852919Z","iopub.execute_input":"2024-01-26T21:35:46.853345Z","iopub.status.idle":"2024-01-26T21:36:02.930436Z","shell.execute_reply.started":"2024-01-26T21:35:46.853311Z","shell.execute_reply":"2024-01-26T21:36:02.928795Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.24.3)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=de5c0506c102c3b090d1ddff82ccbd0f7e44978e0b8668e285b269facc091d39\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_metric\nrouge_metric = load_metric(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:36:02.933131Z","iopub.execute_input":"2024-01-26T21:36:02.933515Z","iopub.status.idle":"2024-01-26T21:36:03.279798Z","shell.execute_reply.started":"2024-01-26T21:36:02.933479Z","shell.execute_reply":"2024-01-26T21:36:03.278935Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb3ea155a83646418d850213b60929e1"}},"metadata":{}}]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:36:03.281293Z","iopub.execute_input":"2024-01-26T21:36:03.282042Z","iopub.status.idle":"2024-01-26T21:36:03.287043Z","shell.execute_reply.started":"2024-01-26T21:36:03.282004Z","shell.execute_reply":"2024-01-26T21:36:03.285925Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[torch]","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:36:04.867847Z","iopub.execute_input":"2024-01-26T21:36:04.868271Z","iopub.status.idle":"2024-01-26T21:36:18.609104Z","shell.execute_reply.started":"2024-01-26T21:36:04.868238Z","shell.execute_reply":"2024-01-26T21:36:18.607693Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.36.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch!=1.12.0,>=1.10 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.0.0)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.25.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install accelerate -U","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:36:18.611762Z","iopub.execute_input":"2024-01-26T21:36:18.612280Z","iopub.status.idle":"2024-01-26T21:36:32.746487Z","shell.execute_reply.started":"2024-01-26T21:36:18.612231Z","shell.execute_reply":"2024-01-26T21:36:32.745111Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\nCollecting accelerate\n  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/a6/b9/44623bdb05595481107153182e7f4b9f2ef9d3b674938ad13842054dcbd8/accelerate-0.26.1-py3-none-any.whl.metadata\n  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.25.0\n    Uninstalling accelerate-0.25.0:\n      Successfully uninstalled accelerate-0.25.0\nSuccessfully installed accelerate-0.26.1\n","output_type":"stream"}]},{"cell_type":"code","source":"# defining training related arguments\nbatch_size = 16\nargs = Seq2SeqTrainingArguments(output_dir=\"weights\",\n                        evaluation_strategy=\"steps\",\n                        per_device_train_batch_size=batch_size,\n                        per_device_eval_batch_size=batch_size,\n                        learning_rate=2e-5,\n                        num_train_epochs=100,\n                        weight_decay=0.01,\n                        save_total_limit=2,\n                        predict_with_generate=True,\n                        fp16 = True,\n                        gradient_accumulation_steps = 6,\n                        eval_steps = 500,\n                        save_steps = 500,\n                        load_best_model_at_end=True,\n                        logging_dir=\"/logs\",\n                        report_to=\"wandb\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:36:32.748461Z","iopub.execute_input":"2024-01-26T21:36:32.748961Z","iopub.status.idle":"2024-01-26T21:36:32.816578Z","shell.execute_reply.started":"2024-01-26T21:36:32.748890Z","shell.execute_reply":"2024-01-26T21:36:32.815536Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Rouge expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n\n    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=False)\n    # Extract a few results\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n\n    # Add mean generated length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:36:32.818960Z","iopub.execute_input":"2024-01-26T21:36:32.819783Z","iopub.status.idle":"2024-01-26T21:36:32.830854Z","shell.execute_reply.started":"2024-01-26T21:36:32.819742Z","shell.execute_reply":"2024-01-26T21:36:32.829671Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# defining trainer using 🤗\ntrainer = Seq2SeqTrainer(model=model,\n                args=args,\n                train_dataset= GrammarDataset(train_dataset, tokenizer),\n                eval_dataset=GrammarDataset(test_dataset, tokenizer),\n                tokenizer=tokenizer,\n                data_collator=data_collator,\n                compute_metrics=compute_metrics)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:36:41.131714Z","iopub.execute_input":"2024-01-26T21:36:41.132761Z","iopub.status.idle":"2024-01-26T21:36:42.369709Z","shell.execute_reply.started":"2024-01-26T21:36:41.132720Z","shell.execute_reply":"2024-01-26T21:36:42.368702Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-26T21:36:45.036058Z","iopub.execute_input":"2024-01-26T21:36:45.036445Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240127_004455-l3fb6x3v</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/osh-hussle/huggingface/runs/l3fb6x3v' target=\"_blank\">dark-fog-1</a></strong> to <a href='https://wandb.ai/osh-hussle/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/osh-hussle/huggingface' target=\"_blank\">https://wandb.ai/osh-hussle/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/osh-hussle/huggingface/runs/l3fb6x3v' target=\"_blank\">https://wandb.ai/osh-hussle/huggingface/runs/l3fb6x3v</a>"},"metadata":{}},{"name":"stderr","text":"You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1446' max='10400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1446/10400 32:40 < 3:22:39, 0.74 it/s, Epoch 13.81/100]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.328500</td>\n      <td>1.827368</td>\n      <td>2.011900</td>\n      <td>0.807900</td>\n      <td>2.006200</td>\n      <td>2.023700</td>\n      <td>10.906200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.746600</td>\n      <td>1.559524</td>\n      <td>2.360400</td>\n      <td>0.858500</td>\n      <td>2.356300</td>\n      <td>2.369200</td>\n      <td>11.781600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.save_model('bangla_gec_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nmodel_name = 'bangla_gec_model'\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n\ndef correct_grammar(input_text,num_return_sequences,input_len):\n  batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=input_len, return_tensors=\"pt\").to(torch_device)\n  translated = model.generate(**batch,max_length=input_len,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n  return tgt_text\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"references,predictions = [],[]\ntest_d = test_df[test_df['Error']==1]\ntest_d_sentence = test_d['Comment'].tolist()\ntest_d_len = test_d['input_token_len'].tolist()\ntest_d_ground = test_d['Correct Form'].tolist()\nimport nltk\nnltk.download('punkt')\nfrom nltk.util import ngrams\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate(reference,hypothesis):\n    # Tokenize sentences into words\n    reference_tokens = nltk.word_tokenize(reference)\n    hypothesis_tokens = nltk.word_tokenize(hypothesis)\n\n    # Create n-grams for reference and hypothesis\n    reference_1grams = list(ngrams(reference_tokens, 1))\n    hypothesis_1grams = list(ngrams(hypothesis_tokens, 1))\n    reference_2grams = list(ngrams(reference_tokens, 2))\n    hypothesis_2grams = list(ngrams(hypothesis_tokens, 2))\n\n    # Calculate ROUGE scores\n    rouge1_precision = len(set(reference_1grams).intersection(hypothesis_1grams)) / len(reference_1grams)\n    rouge1_recall = len(set(reference_1grams).intersection(hypothesis_1grams)) / len(hypothesis_1grams)\n    rouge2_precision = len(set(reference_2grams).intersection(hypothesis_2grams)) / len(reference_2grams)\n    rouge2_recall = len(set(reference_2grams).intersection(hypothesis_2grams)) / len(hypothesis_2grams)\n\n    # Calculate ROUGE-L using NLTK's sentence_bleu function\n    smooth = SmoothingFunction().method4\n    rougeL = sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smooth)\n    d = {\n        \"rouge1_precision\":rouge1_precision,\n        \"rouge1_recall\":rouge1_recall,\n        \"rouge2_precision\":rouge2_precision,\n        \"rouge2_recall\":rouge2_recall,\n        \"rouge_l\":rougeL\n    }\n    return d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.util import ngrams\n\n# Function to calculate ROUGE-1, ROUGE-2, and ROUGE-L scores for a pair of texts\ndef calculate_rouge_scores(reference_tokens, system_tokens):\n    def lcs(X, Y):\n        m, n = len(X), len(Y)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n\n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if X[i - 1] == Y[j - 1]:\n                    dp[i][j] = dp[i - 1][j - 1] + 1\n                else:\n                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n\n        return dp[m][n]\n\n    # Calculate ROUGE-1 (unigram) scores\n    reference_unigrams = set(reference_tokens)\n    system_unigrams = set(system_tokens)\n    overlap_rouge1 = len(reference_unigrams.intersection(system_unigrams))\n    precision_rouge1 = overlap_rouge1 / len(system_unigrams)\n    recall_rouge1 = overlap_rouge1 / len(reference_unigrams)\n    r1_t = 1 if precision_rouge1 + recall_rouge1 == 0 else 0\n    f1_rouge1 = 2 * (precision_rouge1 * recall_rouge1) / (precision_rouge1 + recall_rouge1 + r1_t)\n\n    # Calculate ROUGE-2 (bigram) scores\n    reference_bigrams = set(ngrams(reference_tokens, 2))\n    system_bigrams = set(ngrams(system_tokens, 2))\n    overlap_rouge2 = len(reference_bigrams.intersection(system_bigrams))\n    precision_rouge2 = overlap_rouge2 / len(system_bigrams)\n    recall_rouge2 = overlap_rouge2 / len(reference_bigrams)\n    r2_t = 1 if precision_rouge2 + recall_rouge2 == 0 else 1\n    f1_rouge2 = 2 * (precision_rouge2 * recall_rouge2) / (precision_rouge2 + recall_rouge2 + r2_t)\n\n    # Calculate ROUGE-L scores\n    lcs_length = lcs(reference_tokens, system_tokens)\n    precision_rougeL = lcs_length / len(system_tokens)\n    recall_rougeL = lcs_length / len(reference_tokens)\n    rL_t = 1 if precision_rougeL + recall_rougeL == 0 else 0\n    f1_rougeL = 2 * (precision_rougeL * recall_rougeL) / (precision_rougeL + recall_rougeL + rL_t)\n\n    return {\n        'ROUGE-1 Precision': precision_rouge1,\n        'ROUGE-1 Recall': recall_rouge1,\n        'ROUGE-1 F1': f1_rouge1,\n        'ROUGE-2 Precision': precision_rouge2,\n        'ROUGE-2 Recall': recall_rouge2,\n        'ROUGE-2 F1': f1_rouge2,\n        'ROUGE-L Precision': precision_rougeL,\n        'ROUGE-L Recall': recall_rougeL,\n        'ROUGE-L F1': f1_rougeL,\n    }\n\n# Function to calculate the average of ROUGE scores for an array of text pairs\ndef calculate_average_rouge_scores(reference_texts, system_texts):\n    total_scores = {\n        'ROUGE-1 Precision': 0,\n        'ROUGE-1 Recall': 0,\n        'ROUGE-1 F1': 0,\n        'ROUGE-2 Precision': 0,\n        'ROUGE-2 Recall': 0,\n        'ROUGE-2 F1': 0,\n        'ROUGE-L Precision': 0,\n        'ROUGE-L Recall': 0,\n        'ROUGE-L F1': 0,\n    }\n\n    num_pairs = len(reference_texts)\n\n    for i in range(num_pairs):\n        reference_text = reference_texts[i]\n        system_text = system_texts[i]\n\n        reference_tokens = nltk.word_tokenize(reference_text)\n        system_tokens = nltk.word_tokenize(system_text)\n\n        scores = calculate_rouge_scores(reference_tokens, system_tokens)\n\n        for key, value in scores.items():\n            total_scores[key] += value\n\n    # Calculate the average scores\n    average_scores = {key: value / num_pairs for key, value in total_scores.items()}\n    \n    return average_scores\n\n# Example usage with an array of reference and system texts\nreference_texts = test_d_ground\nsystem_texts = [correct_grammar(test_d_sentence[i],num_return_sequences=2,input_len=test_d_len[i])[0] for i in range(len(test_d_sentence)) ]\n\naverage_scores = calculate_average_rouge_scores(reference_texts, system_texts)\nprint(\"Average ROUGE Scores:\")\nfor key, value in average_scores.items():\n    print(key + \": {:.4f}\".format(value))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# #amar lines ============================================","metadata":{}},{"cell_type":"code","source":"data = []\nfor i in range(len(reference_texts)):\n    row = [reference_texts[i], system_texts[i]]\n    # Add ROUGE scores to the row\n    for key, value in scores[i].items():  # Assuming scores is a list of scores for each pair\n        row.append(value)\n    data.append(row)\n\n# Create the CSV file\nwith open('rouge_resultsT5sml.csv', 'w', newline='') as csvfile:\n    fieldnames = ['Reference Text', 'System Text'] + list(scores[0].keys())  # Use keys from the first score for header\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n    writer.writeheader()\n    writer.writerows({fieldname: value for fieldname, value in row.items()} for row in data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# random data\n\nr = train_df['Comment'].tolist()[:20] + test_df['Comment'].tolist()[:20]\nr_ln = train_df['input_token_len'].tolist()[:20] + test_df['input_token_len'].tolist()[:20]\nreference_texts = r\nfor i in range(len(r)):\n    print(f'[input:   ] {r[i]} and [output:   ] {correct_grammar(r[i],num_return_sequences=2,input_len=r_ln[i])[0]}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# error data\n\nimport time\n\nr = (test_df[test_df['Error']==1])['Comment'].tolist()\nr_ln = (test_df[test_df['Error']==1])['input_token_len'].tolist()\nreference_texts = r\nstart_time = time.time()\nfor i in range(len(r)):\n    x = r[i]\n    y = correct_grammar(r[i],num_return_sequences=2,input_len=r_ln[i])[0]\n#     print(f'[input:   ] {r[i]} and [output:   ] {correct_grammar(r[i],num_return_sequences=2,input_len=r_ln[i])[0]}')\nend_time = time.time()\nnum_iterations = len(r)\naverage_inference_time = (end_time - start_time) / num_iterations\nprint(f\"Total Inference Time: {end_time-start_time}\")\nprint(f\"Average Inference Time: {average_inference_time:.4f} seconds\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r_gn = (test_df[test_df['Error']==1])['Correct Form'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_score,predicted_sentences = [],[]\nfor i in range(len(r_gn)):\n    x = r_gn[i]\n    y = correct_grammar(r[i],num_return_sequences=2,input_len=r_ln[i])[0]\n    predicted_sentences.append(y)\n    rouge_score_v = calculate_rouge_scores(x, y)\n    predicted_score.append(rouge_score_v['ROUGE-L F1'])\n# test_df['ROUGE-L F1'] = predicted_sentence\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e_df = test_df[test_df['Error']==1]\ne_df['ROUGE-L F1'] = predicted_score\ne_df['Predicted Form'] = predicted_sentences\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e_df.sort_values(by='ROUGE-L F1', inplace=True,ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e_df.head(20)[1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e_df.tail(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}