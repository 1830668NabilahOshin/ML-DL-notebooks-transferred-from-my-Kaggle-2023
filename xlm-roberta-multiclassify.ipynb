{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1382412,"sourceType":"datasetVersion","datasetId":806606},{"sourceId":7449449,"sourceType":"datasetVersion","datasetId":4336196}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModelForMaskedLM,XLMRobertaForSequenceClassification","metadata":{"id":"2e0f75c3","outputId":"1968999e-09e5-4f8d-fb47-377c1b235879","papermill":{"duration":13.349991,"end_time":"2023-07-25T16:06:48.929213","exception":false,"start_time":"2023-07-25T16:06:35.579222","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-22T04:53:04.663663Z","iopub.execute_input":"2024-01-22T04:53:04.664331Z","iopub.status.idle":"2024-01-22T04:53:12.452781Z","shell.execute_reply.started":"2024-01-22T04:53:04.664286Z","shell.execute_reply":"2024-01-22T04:53:12.451879Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"model_name = 'xlm-roberta-base'\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\nmodel = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\",num_labels=5)\n# Define device (CPU or GPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"id":"4913d932","outputId":"5eb3dd9b-5b04-4803-ae2a-b601b8da7ba0","papermill":{"duration":4.173444,"end_time":"2023-07-25T16:06:53.106774","exception":false,"start_time":"2023-07-25T16:06:48.933330","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-22T04:53:37.181810Z","iopub.execute_input":"2024-01-22T04:53:37.182448Z","iopub.status.idle":"2024-01-22T04:53:46.161777Z","shell.execute_reply.started":"2024-01-22T04:53:37.182405Z","shell.execute_reply":"2024-01-22T04:53:46.160545Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b7e68189d3a411aae8672f57ab837d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43c8cb128bbd422a89d0c1da82a68923"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8b9d3fdbbcf46f895b8c319fada6cc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dd541148c1f40f0ac8acba1f06aa1be"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ntrain_url = '/kaggle/input/80-20ratiofinal/train8020.csv'\ntest_url = '/kaggle/input/80-20ratiofinal/test8020.csv'\ndf_train = pd.read_csv(train_url)\ndf_test = pd.read_csv(test_url)\nstop_words_df = pd.read_excel('/kaggle/input/bangla-stopwords/stopwords_bangla.xlsx',index_col=False)\nSTOPWORDS = set([word.strip() for word in stop_words_df['words']])\n\nimport re\ndef preprocess(x):\n    html_pattern = re.compile('<.*?>')\n    x = html_pattern.sub(r'', x)\n    x = \" \".join([word for word in str(x).split() if word not in STOPWORDS])\n    return x\ndf_train['Comment'] = df_train['Comment'].apply(lambda x: preprocess(x))\ndf_test['Comment'] = df_test['Comment'].apply(lambda x:preprocess(x))\n\nimport numpy as np\nallcats = set(df_train['Category'].dropna().tolist())\nallcats.add('Correct')\nlabeldict = {}\ncounter = 0\nfor i in allcats:\n    labeldict[i] = counter\n    counter += 1\nlabeldict\n\ndef manage(x):\n    if x in labeldict:\n        return labeldict[x]\n    else:\n        return labeldict['Correct']\ndf_train['Category'] = df_train['Category'].apply(lambda x:manage(x))\ndf_test['Category'] = df_test['Category'].apply(lambda x:manage(x))\n\ndata_no = 5\n\n# Prepare the training data\ntrain_texts = df_train['Comment'].tolist()\ntrain_labels = df_train['Category'].tolist()\n\ntest_texts = df_test['Comment'].tolist()\ntest_labels = df_test['Category'].tolist()","metadata":{"id":"b0560ae3","papermill":{"duration":0.132745,"end_time":"2023-07-25T16:06:53.244675","exception":false,"start_time":"2023-07-25T16:06:53.111930","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-22T04:57:16.065993Z","iopub.execute_input":"2024-01-22T04:57:16.066956Z","iopub.status.idle":"2024-01-22T04:57:16.730522Z","shell.execute_reply.started":"2024-01-22T04:57:16.066913Z","shell.execute_reply":"2024-01-22T04:57:16.729288Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Tokenize and encode the training texts\ntrain_encodings = tokenizer(train_texts, truncation=True, max_length=128,\n        padding='max_length', return_tensors = 'pt')\n\n# Convert the labels to tensors\ntrain_labels = torch.tensor(train_labels)\n\n# Create a PyTorch dataset\ntrain_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'],\n                                               train_encodings['attention_mask'],\n                                               train_labels)\n\n# Create a data loader\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n\nmodel = model.to(device)","metadata":{"id":"c68abf8e","outputId":"fc006e00-bc36-49f7-9041-3cc6a300504b","papermill":{"duration":11.003131,"end_time":"2023-07-25T16:07:04.276102","exception":false,"start_time":"2023-07-25T16:06:53.272971","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-22T04:57:24.973124Z","iopub.execute_input":"2024-01-22T04:57:24.973533Z","iopub.status.idle":"2024-01-22T04:57:28.473093Z","shell.execute_reply.started":"2024-01-22T04:57:24.973503Z","shell.execute_reply":"2024-01-22T04:57:28.471699Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score\n\n# Set the model to training mode\nmodel.train()\n\n# Define the optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\nlosses = []\naccuracies = []  # To store accuracy per epoch\nnum_epochs = 5\n# Training loop\nfor epoch in tqdm(range(num_epochs)):  # Number of training epochs\n    running_loss = 0.0\n    predicted_labels = []  # To store predicted labels for accuracy calculation\n    true_labels = []  # To store true labels for accuracy calculation\n\n    for batch in tqdm(train_loader):\n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels = batch[2].to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # Convert logits to predicted labels\n        _, predicted = torch.max(logits, dim=1)\n        predicted_labels.extend(predicted.cpu().tolist())\n        true_labels.extend(labels.cpu().tolist())\n\n    epoch_loss = running_loss / len(train_loader)\n    losses.append(epoch_loss)\n\n    # Calculate and store accuracy\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    accuracies.append(accuracy)\n\n    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {accuracy:.4f}')\n\n# Save the model\ntorch.save(model.state_dict(), 'model.pth')\n","metadata":{"id":"eedb3a9c","outputId":"81f0d82e-5d90-4271-e5cf-71638bf8bc98","papermill":{"duration":668.41198,"end_time":"2023-07-25T16:18:12.693572","exception":false,"start_time":"2023-07-25T16:07:04.281592","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-01-22T04:57:32.789350Z","iopub.execute_input":"2024-01-22T04:57:32.790638Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"270ccac707404d3aa39c4e2e0165b490"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1256 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"375e117b48fa4e25b03ee5b60f71a400"}},"metadata":{}}]},{"cell_type":"code","source":"#dgfdgdfgdgffdgdfd1212jhkhk","metadata":{"papermill":{"duration":0.014424,"end_time":"2023-07-25T16:18:12.713695","exception":false,"start_time":"2023-07-25T16:18:12.699271","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-11T01:04:08.353971Z","iopub.execute_input":"2023-09-11T01:04:08.354342Z","iopub.status.idle":"2023-09-11T01:04:08.359834Z","shell.execute_reply.started":"2023-09-11T01:04:08.354310Z","shell.execute_reply":"2023-09-11T01:04:08.358915Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n\ndef predict_labels(text):\n    train_encodings = tokenizer(text, truncation=True, max_length=128,\n        padding='max_length', return_tensors = 'pt')\n    input_ids = train_encodings['input_ids'].to(device)\n    attention_mask = train_encodings['attention_mask'].to(device)\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Disable gradient calculation\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n\n    logits = outputs.logits\n    probabilities = torch.softmax(logits, dim=1)\n    predicted_class = torch.argmax(probabilities, dim=1)\n    \n\n    return predicted_class.item(), probabilities[:,1].item()","metadata":{"id":"1243273c","papermill":{"duration":0.69644,"end_time":"2023-07-25T16:18:13.415572","exception":false,"start_time":"2023-07-25T16:18:12.719132","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-11T01:04:08.361376Z","iopub.execute_input":"2023-09-11T01:04:08.362076Z","iopub.status.idle":"2023-09-11T01:04:08.373086Z","shell.execute_reply.started":"2023-09-11T01:04:08.362043Z","shell.execute_reply":"2023-09-11T01:04:08.372244Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"predicted_labels = []\npredicted_probs = []\nfor text in tqdm(test_texts):\n    predicted_label, prob = predict_labels(text)\n    predicted_labels.append(predicted_label)\n    predicted_probs.append(prob)\n\n# Calculate accuracy and F1 score\naccuracy = accuracy_score(test_labels, predicted_labels)\n# f1 = f1_score(test_labels, predicted_labels)\n# roc_auc = roc_auc_score(test_labels, predicted_probs)\n\nprint('Accuracy:', accuracy)\n# print('F1 Score:', f1)\n# print('ROC-AUC:', roc_auc)","metadata":{"id":"d57fe764","outputId":"d7e9495a-7096-413f-cc6e-0f3e69cdc327","papermill":{"duration":56.127329,"end_time":"2023-07-25T16:19:09.548653","exception":false,"start_time":"2023-07-25T16:18:13.421324","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-11T01:04:08.374538Z","iopub.execute_input":"2023-09-11T01:04:08.374893Z","iopub.status.idle":"2023-09-11T01:04:40.045665Z","shell.execute_reply.started":"2023-09-11T01:04:08.374844Z","shell.execute_reply":"2023-09-11T01:04:40.044489Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2010 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b67e94998b24433daf8795607eba1afd"}},"metadata":{}},{"name":"stdout","text":"Accuracy: 0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Accuracy:', accuracy)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-09-11T01:04:40.047066Z","iopub.execute_input":"2023-09-11T01:04:40.047443Z","iopub.status.idle":"2023-09-11T01:04:40.053189Z","shell.execute_reply.started":"2023-09-11T01:04:40.047408Z","shell.execute_reply":"2023-09-11T01:04:40.052114Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Accuracy: 0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, classification_report\n\nprint('\\nThe Classification Report is as follows\\n')\nprint(classification_report(test_labels, predicted_labels, digits = 4))","metadata":{"papermill":{"duration":0.031899,"end_time":"2023-07-25T16:19:09.586107","exception":false,"start_time":"2023-07-25T16:19:09.554208","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-09-11T01:04:40.054841Z","iopub.execute_input":"2023-09-11T01:04:40.055462Z","iopub.status.idle":"2023-09-11T01:04:40.090229Z","shell.execute_reply.started":"2023-09-11T01:04:40.055429Z","shell.execute_reply":"2023-09-11T01:04:40.089099Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\nThe Classification Report is as follows\n\n              precision    recall  f1-score   support\n\n           0     0.7631    0.8323    0.7962      1157\n           1     0.0000    0.0000    0.0000       128\n           2     0.2727    0.0435    0.0750        69\n           3     0.5934    0.6827    0.6349       498\n           4     0.6159    0.6392    0.6273       158\n\n    accuracy                         0.7000      2010\n   macro avg     0.4490    0.4396    0.4267      2010\nweighted avg     0.6440    0.7000    0.6675      2010\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}